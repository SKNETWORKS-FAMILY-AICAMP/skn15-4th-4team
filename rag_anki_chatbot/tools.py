# tools.py
import json
import requests
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PDFPlumberLoader, Docx2txtLoader
from dotenv import load_dotenv
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_openai import OpenAIEmbeddings
from langchain_postgres import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter
import time
import chardet
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

load_dotenv()

ANKI_CONNECT_URL = "http://192.168.160.1:8765"
tavily_search_tool = TavilySearchResults(max_results=3)

# ğŸ”¥ ê°œì„ : ì„¸ì…˜ë³„ ë¬¸ì„œ ì €ì¥ì†Œ ê´€ë¦¬
class DocumentManager:
    """ë¬¸ì„œ ì—…ë¡œë“œì™€ ê²€ìƒ‰ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.stores = {}  # session_id -> FAISS store ë§¤í•‘
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    def upload_document(self, file_path: str, file_name: str, session_id: str = "default") -> str:
        """ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±"""
        try:
            print(f"ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ(ì„¸ì…˜: {session_id}): {file_name}")

            ext = (file_name.rsplit(".", 1)[-1] if "." in file_name else "").lower()
            texts = []

            # íŒŒì¼ í˜•ì‹ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if ext == "pdf":
                loader = PDFPlumberLoader(file_path)
                docs = loader.load()
                texts = [d.page_content for d in docs if d.page_content and d.page_content.strip()]
            elif ext in ("docx", "doc"):
                loader = Docx2txtLoader(file_path)
                docs = loader.load()
                texts = [d.page_content for d in docs if d.page_content and d.page_content.strip()]
            else:
                # txt, md ë“± ì¼ë°˜ í…ìŠ¤íŠ¸
                with open(file_path, 'rb') as f:
                    raw = f.read()
                enc = chardet.detect(raw)['encoding'] or 'utf-8'
                content = raw.decode(enc, errors='replace').replace('\x00', '')
                texts = [content] if content.strip() else []

            if not texts:
                return f"âŒ '{file_name}'ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # í…ìŠ¤íŠ¸ ë¶„í• 
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=600,
                chunk_overlap=150,
                separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
            )
            chunks = []
            for t in texts:
                chunks.extend(splitter.split_text(t))

            if not chunks:
                return f"âŒ '{file_name}'ì—ì„œ ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤."

            metadatas = [{"source": file_name, "chunk_id": i} for i in range(len(chunks))]

            # FAISS ì¸ë±ìŠ¤ ìƒì„±
            vs = FAISS.from_texts(texts=chunks, embedding=self.embeddings, metadatas=metadatas)

            # ì„¸ì…˜ë³„ ì €ì¥ì†Œì— ì €ì¥
            if session_id not in self.stores:
                self.stores[session_id] = {}
            
            self.stores[session_id][file_name] = vs
            
            return f"âœ… **'{file_name}' ì—…ë¡œë“œ ì™„ë£Œ!** (ì„¸ì…˜: {session_id}, ì²­í¬: {len(chunks)}ê°œ)"

        except Exception as e:
            return f"ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def search_documents(self, query: str, session_id: str = "default") -> str:
        """ì„¸ì…˜ë³„ë¡œ ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê²€ìƒ‰"""
        try:
            print(f"ğŸ“„ ë¬¸ì„œ ê²€ìƒ‰ (ì„¸ì…˜: {session_id}): '{query}'")

            if session_id not in self.stores or not self.stores[session_id]:
                return "ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

            # ê° íŒŒì¼ ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
            k_per_file = 5
            gathered = []
            
            for fname, store in self.stores[session_id].items():
                try:
                    docs = store.as_retriever(search_kwargs={"k": k_per_file}).get_relevant_documents(query)
                    gathered.extend(docs)
                except Exception as e:
                    print(f"   âŒ '{fname}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            if not gathered:
                return f"'{query}'ì— ëŒ€í•œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # ì ìˆ˜ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            q_emb = self.embeddings.embed_query(query)
            scored = []
            
            for i, d in enumerate(gathered):
                try:
                    d_emb = self.embeddings.embed_query(d.page_content)
                    sim = cosine_similarity([q_emb], [d_emb])[0][0]
                    scored.append({
                        "document": d,
                        "score": float(sim),
                        "source": d.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"),
                        "chunk_id": d.metadata.get("chunk_id", i),
                    })
                except Exception as e:
                    print(f"   âŒ ì ìˆ˜ê³„ì‚° ì‹¤íŒ¨: {e}")

            if not scored:
                return "ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

            # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            scored.sort(key=lambda x: x['score'], reverse=True)

            # ì„ê³„ê°’ ì ìš©
            PRIMARY = 0.3
            FALLBACK = 0.1
            picked = [x for x in scored if x['score'] >= PRIMARY] or \
                     [x for x in scored if x['score'] >= FALLBACK] or \
                     scored[:5]

            # ì¤‘ë³µ ì œê±°
            unique = self._remove_duplicates(picked)
            top = unique[:5]
            
            if not top:
                return f"'{query}'ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # ê²°ê³¼ í¬ë§·íŒ…
            parts = []
            scores = [x['score'] for x in scored]
            
            for i, x in enumerate(top, 1):
                level = "ë†’ìŒ" if x['score'] >= 0.5 else "ì¤‘ê°„" if x['score'] >= 0.3 else "ë‚®ìŒ"
                content = x['document'].page_content.strip()
                parts.append(
                    f"ğŸ“‹ **ë¬¸ì„œ {i}** (ê´€ë ¨ì„±: {level}) - {x['source']} (ì²­í¬ {x['chunk_id']})\n{content}\n"
                )

            return (
                f"ğŸ” **'{query}' ê²€ìƒ‰ ê²°ê³¼** ({len(parts)}ê°œ ê´€ë ¨ ë¬¸ì„œ)\n\n" +
                "\n".join(parts) +
                f"\nğŸ“Œ **ê²€ìƒ‰ ì •ë³´**: í›„ë³´ {len(scored)}ê°œ ì¤‘ ìƒìœ„ {len(top)}ê°œ ì„ ë³„\n" +
                f"ğŸ“Š **ì ìˆ˜ ë²”ìœ„**: {max(scores):.3f} ~ {min(scores):.3f} (í‰ê·  {sum(scores)/len(scores):.3f})"
            )

        except Exception as e:
            return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def _remove_duplicates(self, scored_docs: list) -> list:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±°"""
        if len(scored_docs) <= 1:
            return scored_docs
        
        unique_docs = []
        seen_contents = set()
        
        for doc_info in scored_docs:
            content = doc_info['document'].page_content
            content_key = content[:100].strip().lower()
            
            if content_key not in seen_contents:
                unique_docs.append(doc_info)
                seen_contents.add(content_key)
        
        return unique_docs

    def get_document_status(self, session_id: str = "default") -> str:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ í˜„í™© í™•ì¸"""
        try:
            if session_id not in self.stores or not self.stores[session_id]:
                return "âŒ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
            
            total_chunks = 0
            status_msg = f"ğŸ“Š **ì—…ë¡œë“œëœ ë¬¸ì„œ í˜„í™©** (ì„¸ì…˜: {session_id})\n\n"
            
            for fname, store in self.stores[session_id].items():
                # ê° íŒŒì¼ì˜ ì²­í¬ ìˆ˜ ê³„ì‚° (ì„ì‹œë¡œ 1ê°œ ë¬¸ì„œ ê²€ìƒ‰í•´ì„œ ì „ì²´ ì¸ë±ìŠ¤ í¬ê¸° ì¶”ì •)
                try:
                    sample_docs = store.as_retriever(search_kwargs={"k": 100}).get_relevant_documents("*")
                    chunk_count = len(sample_docs)
                    total_chunks += chunk_count
                    status_msg += f"ğŸ“„ {fname}: {chunk_count}ê°œ ì²­í¬\n"
                    
                    # ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
                    if sample_docs:
                        preview = sample_docs[0].page_content[:50].replace('\n', ' ')
                        status_msg += f"   â†’ ìƒ˜í”Œ: {preview}...\n"
                        
                except Exception as e:
                    status_msg += f"ğŸ“„ {fname}: ì˜¤ë¥˜ ({e})\n"
            
            status_msg += f"\n**ì´ ì²­í¬ ìˆ˜**: {total_chunks}ê°œ"
            return status_msg
            
        except Exception as e:
            return f"âŒ ë¬¸ì„œ í˜„í™© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}"

# ğŸ”¥ ì „ì—­ ë¬¸ì„œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
doc_manager = DocumentManager()

# ê¸°ì¡´ Anki ê´€ë ¨ DBëŠ” ìœ ì§€ (ì´ì „ ëŒ€í™” ê²€ìƒ‰ìš©)
VECTORSTORE_DSN = "postgresql+psycopg2://play:123@localhost:5432/play"
ANKI_COLLECTION_NAME = "play_anki_cards"

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
anki_vector_store = PGVector(
    connection=VECTORSTORE_DSN,
    embeddings=embeddings,
    collection_name=ANKI_COLLECTION_NAME,
    use_jsonb=True,
)

def web_search(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ ë„êµ¬"""
    try:
        print(f"ğŸ–¥ï¸  ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {query}")
        results = tavily_search_tool.invoke({"query": query})
        return f"'{query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.\n\n{results}"
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def document_search(query: str, session_id: str = "default") -> str:
    """ë¬¸ì„œ ê²€ìƒ‰ - ê°œì„ ëœ DocumentManager ì‚¬ìš©"""
    return doc_manager.search_documents(query, session_id)

def upload_document(file_path: str, file_name: str, session_id: str = "default") -> str:
    """ë¬¸ì„œ ì—…ë¡œë“œ - ê°œì„ ëœ DocumentManager ì‚¬ìš©"""
    return doc_manager.upload_document(file_path, file_name, session_id)

def check_document_status(session_id: str = "default") -> str:
    """ë¬¸ì„œ í˜„í™© í™•ì¸ - ê°œì„ ëœ DocumentManager ì‚¬ìš©"""
    return doc_manager.get_document_status(session_id)

def anki_card_saver(front: str, back: str, deck: str = "ê¸°ë³¸", tags: list = None) -> str:
    """
    AnkiConnect APIë¥¼ ì‚¬ìš©í•˜ì—¬ Anki ë°ìŠ¤í¬í†± í”„ë¡œê·¸ë¨ì— ìƒˆ ì¹´ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    print(f"ğŸƒ Anki ì¹´ë“œ ì €ì¥ ì‹œë„: {front[:30]}...")
    
    def anki_request(action, **params):
        return {'action': action, 'version': 6, 'params': params}

    try:
        # 1. ì¤‘ë³µ ì¹´ë“œ í™•ì¸
        front_keywords = front.replace('\n', ' ')[:50]
        query = f'deck:"{deck}" front:*{front_keywords}*'
        find_payload = anki_request('findNotes', query=query)
        response = requests.post(ANKI_CONNECT_URL, json=find_payload)
        response.raise_for_status()
        
        print(f"   -> Anki 'findNotes' ì‘ë‹µ: {response.json()}")
        
        existing_notes = response.json().get('result', [])
        if existing_notes:
            message = f"ìœ ì‚¬í•œ ì¹´ë“œê°€ '{deck}' ë±ì— ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìƒˆë¡œ ì¶”ê°€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            print(f"   -> {message}")
            return message

        # 2. ìƒˆ ë…¸íŠ¸(ì¹´ë“œ) ì¶”ê°€
        note_params = {
            'note': {
                'deckName': deck,
                'modelName': 'Basic',
                'fields': {
                    'Front': front,
                    'Back': back.replace("\n", "<br>")
                },
                'tags': tags if tags else []
            }
        }
        
        add_payload = anki_request('addNote', **note_params)
        response = requests.post(ANKI_CONNECT_URL, json=add_payload)
        response.raise_for_status()

        print(f"   -> Anki 'addNote' ì‘ë‹µ: {response.json()}")

        response_data = response.json()
        if error := response_data.get('error'):
            raise Exception(f"AnkiConnect ì˜¤ë¥˜: {error}")
        
        note_id = response_data.get('result')
        
        # ğŸ”¥ ê°œì„ : Anki ì¹´ë“œë¥¼ ë²¡í„°DBì—ë„ ì €ì¥ (ì´ì „ ëŒ€í™” ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡)
        try:
            anki_vector_store.add_texts(
                texts=[f"Anki ì¹´ë“œ: {front}\në‹µë³€: {back}"], 
                metadatas=[{"type": "anki_card", "deck": deck, "anki_id": note_id}]
            )
        except Exception as db_error:
            print(f"   âš ï¸ ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: {db_error}")
        
        message = f"âœ… Anki ì¹´ë“œë¥¼ '{deck}' ë±ì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. (ID: {note_id})"
        print(f"   -> {message}")
        return message

    except requests.exceptions.ConnectionError:
        error_msg = "âŒ AnkiConnectì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Anki í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ ì¤‘ì´ê³  AnkiConnect ì• ë“œì˜¨ì´ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        print(f"   -> {error_msg}")
        return error_msg
    except Exception as e:
        error_msg = f"âŒ Anki ì¹´ë“œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(f"   -> {error_msg}")
        return error_msg

def list_anki_decks() -> str:
    """Ankiì˜ ëª¨ë“  ë± ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬"""
    def anki_request(action, **params):
        return {'action': action, 'version': 6, 'params': params}
    
    try:
        payload = anki_request('deckNames')
        response = requests.post(ANKI_CONNECT_URL, json=payload)
        response.raise_for_status()
        
        decks = response.json().get('result', [])
        if decks:
            return f"ì‚¬ìš© ê°€ëŠ¥í•œ Anki ë± ëª©ë¡:\n" + "\n".join([f"- {deck}" for deck in decks])
        else:
            return "ì‚¬ìš© ê°€ëŠ¥í•œ ë±ì´ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        return f"ë± ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"